Loaded model from /homes/gws/lxh22/models/Qwen2.5-Math-1.5B
Model config: Qwen2Config {
  "_name_or_path": "/homes/gws/lxh22/models/Qwen2.5-Math-1.5B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 4096,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "use_mrope": false,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Training arguments:
  Num examples = 10145
  Per device batch size = 8
  Total train batch size = 64
  Total optimization steps = 3000
  Number of epochs = 19
  Expected effective batch size = 64
Step 50: loss=0.1376, lr=2.00e-05
Step 50: eval_loss=0.1250
Step 100: loss=0.1276, lr=2.00e-05
Step 100: eval_loss=0.1205
[2025-06-24 01:23:35,447] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /homes/gws/lxh22/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-100
Step 150: loss=0.1198, lr=2.00e-05
Step 150: eval_loss=0.1162
Step 200: loss=0.1125, lr=2.00e-05
Step 200: eval_loss=0.1126
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-200
Step 250: loss=0.1147, lr=2.00e-05
Step 250: eval_loss=0.1111
Step 300: loss=0.1096, lr=2.00e-05
Step 300: eval_loss=0.1107
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-300
Step 350: loss=0.1111, lr=2.00e-05
Step 350: eval_loss=0.1105
Step 400: loss=0.1090, lr=2.00e-05
Step 400: eval_loss=0.1105
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-400
Step 450: loss=0.1097, lr=2.00e-05
Step 450: eval_loss=0.1104
Step 500: loss=0.1110, lr=2.00e-05
Step 500: eval_loss=0.1105
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-500
Step 550: loss=0.1064, lr=2.00e-05
Step 550: eval_loss=0.1105
Step 600: loss=0.1093, lr=2.00e-05
Step 600: eval_loss=0.1102
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-600
Step 650: loss=0.1065, lr=2.00e-05
Step 650: eval_loss=0.1105
Step 700: loss=0.1079, lr=2.00e-05
Step 700: eval_loss=0.1104
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-700
Step 750: loss=0.1058, lr=2.00e-05
Step 750: eval_loss=0.1105
Step 800: loss=0.1088, lr=2.00e-05
Step 800: eval_loss=0.1105
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-800
Step 850: loss=0.1053, lr=2.00e-05
Step 850: eval_loss=0.1107
Step 900: loss=0.1068, lr=2.00e-05
Step 900: eval_loss=0.1104
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-900
Step 950: loss=0.1051, lr=2.00e-05
Step 950: eval_loss=0.1105
Step 1000: loss=0.1047, lr=2.00e-05
Step 1000: eval_loss=0.1108
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1000
Step 1050: loss=0.1064, lr=2.00e-05
Step 1050: eval_loss=0.1107
Step 1100: loss=0.1068, lr=2.00e-05
Step 1100: eval_loss=0.1107
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1100
Step 1150: loss=0.1063, lr=2.00e-05
Step 1150: eval_loss=0.1109
Step 1200: loss=0.1040, lr=2.00e-05
Step 1200: eval_loss=0.1110
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1200
Step 1250: loss=0.1046, lr=2.00e-05
Step 1250: eval_loss=0.1108
Step 1300: loss=0.1008, lr=2.00e-05
Step 1300: eval_loss=0.1109
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1300
Step 1350: loss=0.1052, lr=2.00e-05
Step 1350: eval_loss=0.1110
Step 1400: loss=0.1032, lr=2.00e-05
Step 1400: eval_loss=0.1108
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1400
Step 1450: loss=0.1033, lr=2.00e-05
Step 1450: eval_loss=0.1114
Step 1500: loss=0.1015, lr=2.00e-05
Step 1500: eval_loss=0.1113
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1500
Step 1550: loss=0.1058, lr=2.00e-05
Step 1550: eval_loss=0.1113
Step 1600: loss=0.1019, lr=2.00e-05
Step 1600: eval_loss=0.1116
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1600
Step 1650: loss=0.1018, lr=2.00e-05
Step 1650: eval_loss=0.1114
Step 1700: loss=0.1014, lr=2.00e-05
Step 1700: eval_loss=0.1115
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1700
Step 1750: loss=0.0966, lr=2.00e-05
Step 1750: eval_loss=0.1118
Step 1800: loss=0.1026, lr=2.00e-05
Step 1800: eval_loss=0.1117
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1800
Step 1850: loss=0.0992, lr=2.00e-05
Step 1850: eval_loss=0.1120
Step 1900: loss=0.1018, lr=2.00e-05
Step 1900: eval_loss=0.1120
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-1900
Step 1950: loss=0.1009, lr=2.00e-05
Step 1950: eval_loss=0.1122
Step 2000: loss=0.1017, lr=2.00e-05
Step 2000: eval_loss=0.1122
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2000
Step 2050: loss=0.1014, lr=2.00e-05
Step 2050: eval_loss=0.1119
Step 2100: loss=0.0984, lr=2.00e-05
Step 2100: eval_loss=0.1128
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2100
Step 2150: loss=0.0989, lr=2.00e-05
Step 2150: eval_loss=0.1128
Step 2200: loss=0.1006, lr=2.00e-05
Step 2200: eval_loss=0.1127
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2200
Step 2250: loss=0.0981, lr=2.00e-05
Step 2250: eval_loss=0.1127
Step 2300: loss=0.0970, lr=2.00e-05
Step 2300: eval_loss=0.1130
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2300
Step 2350: loss=0.0990, lr=2.00e-05
Step 2350: eval_loss=0.1132
Step 2400: loss=0.0984, lr=2.00e-05
Step 2400: eval_loss=0.1136
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2400
Step 2450: loss=0.0981, lr=2.00e-05
Step 2450: eval_loss=0.1137
Step 2500: loss=0.0957, lr=2.00e-05
Step 2500: eval_loss=0.1137
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2500
Step 2550: loss=0.0969, lr=2.00e-05
Step 2550: eval_loss=0.1142
Step 2600: loss=0.0956, lr=2.00e-05
Step 2600: eval_loss=0.1143
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2600
Step 2650: loss=0.0947, lr=2.00e-05
Step 2650: eval_loss=0.1143
Step 2700: loss=0.0950, lr=2.00e-05
Step 2700: eval_loss=0.1148
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2700
Step 2750: loss=0.0969, lr=2.00e-05
Step 2750: eval_loss=0.1150
Step 2800: loss=0.0967, lr=2.00e-05
Step 2800: eval_loss=0.1151
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2800
Step 2850: loss=0.0947, lr=2.00e-05
Step 2850: eval_loss=0.1152
Step 2900: loss=0.0961, lr=2.00e-05
Step 2900: eval_loss=0.1159
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-2900
Step 2950: loss=0.0986, lr=2.00e-05
Step 2950: eval_loss=0.1157
Step 3000: loss=0.0968, lr=2.00e-05
Step 3000: eval_loss=0.1155
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-3000
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-final
