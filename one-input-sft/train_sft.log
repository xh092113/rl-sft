Loaded model from /homes/gws/lxh22/models/Qwen2.5-Math-1.5B
Model config: Qwen2Config {
  "_name_or_path": "/homes/gws/lxh22/models/Qwen2.5-Math-1.5B",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 4096,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 10000,
  "sliding_window": null,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "use_mrope": false,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Training arguments:
  Num examples = 10145
  Per device batch size = 8
  Total train batch size = 64
  Total optimization steps = 100
  Number of epochs = 1
  Expected effective batch size = 64
Step 5: loss=0.1589, lr=2.00e-05
Step 5: eval_loss=0.1349
Step 10: loss=0.1493, lr=2.00e-05
Step 10: eval_loss=0.1314
[2025-06-20 17:44:28,748] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /homes/gws/lxh22/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-10
Step 15: loss=0.1418, lr=2.00e-05
Step 15: eval_loss=0.1302
Step 20: loss=0.1270, lr=2.00e-05
Step 20: eval_loss=0.1287
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-20
Step 25: loss=0.1331, lr=2.00e-05
Step 25: eval_loss=0.1279
Step 30: loss=0.1325, lr=2.00e-05
Step 30: eval_loss=0.1271
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-30
Step 35: loss=0.1303, lr=2.00e-05
Step 35: eval_loss=0.1264
Step 40: loss=0.1348, lr=2.00e-05
Step 40: eval_loss=0.1260
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-40
Step 45: loss=0.1312, lr=2.00e-05
Step 45: eval_loss=0.1255
Step 50: loss=0.1370, lr=2.00e-05
Step 50: eval_loss=0.1251
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-50
Step 55: loss=0.1317, lr=2.00e-05
Step 55: eval_loss=0.1245
Step 60: loss=0.1268, lr=2.00e-05
Step 60: eval_loss=0.1240
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-60
Step 65: loss=0.1322, lr=2.00e-05
Step 65: eval_loss=0.1237
Step 70: loss=0.1243, lr=2.00e-05
Step 70: eval_loss=0.1234
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-70
Step 75: loss=0.1243, lr=2.00e-05
Step 75: eval_loss=0.1227
Step 80: loss=0.1262, lr=2.00e-05
Step 80: eval_loss=0.1223
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-80
Step 85: loss=0.1325, lr=2.00e-05
Step 85: eval_loss=0.1219
Step 90: loss=0.1298, lr=2.00e-05
Step 90: eval_loss=0.1214
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-90
Step 95: loss=0.1302, lr=2.00e-05
Step 95: eval_loss=0.1208
Step 100: loss=0.1178, lr=2.00e-05
Step 100: eval_loss=0.1205
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-100
Checkpoint saved to save/Qwen2.5-Math-1.5B/step-final
