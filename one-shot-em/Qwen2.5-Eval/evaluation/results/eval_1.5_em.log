Random seed set as 0
INFO 06-25 22:31:33 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 22:31:33 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-shot-em/checkpoints/Qwen2.5-Math-1.5B/one_shot_1.5b_t0.5/final', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-shot-em/checkpoints/Qwen2.5-Math-1.5B/one_shot_1.5b_t0.5/final', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-shot-em/checkpoints/Qwen2.5-Math-1.5B/one_shot_1.5b_t0.5/final, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 22:31:34 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 22:31:34 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=1033405)[0;0m INFO 06-25 22:31:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=1033403)[0;0m [1;36m(VllmWorkerProcess pid=1033404)[0;0m INFO 06-25 22:31:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 22:31:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=1033404)[0;0m INFO 06-25 22:31:37 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 22:31:37 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1033403)[0;0m INFO 06-25 22:31:37 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 22:31:37 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=1033405)[0;0m [1;36m(VllmWorkerProcess pid=1033404)[0;0m INFO 06-25 22:31:37 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1033403)[0;0m INFO 06-25 22:31:37 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 22:31:37 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=1033405)[0;0m INFO 06-25 22:31:37 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=1033403)[0;0m [1;36m(VllmWorkerProcess pid=1033404)[0;0m WARNING 06-25 22:31:38 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 22:31:38 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 22:31:38 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=1033405)[0;0m WARNING 06-25 22:31:38 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 22:31:38 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f7309a23460>, local_subscribe_port=50147, remote_subscribe_port=None)
INFO 06-25 22:31:38 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-shot-em/checkpoints/Qwen2.5-Math-1.5B/one_shot_1.5b_t0.5/final...
[1;36m(VllmWorkerProcess pid=1033404)[0;0m INFO 06-25 22:31:38 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-shot-em/checkpoints/Qwen2.5-Math-1.5B/one_shot_1.5b_t0.5/final...
[1;36m(VllmWorkerProcess pid=1033403)[0;0m INFO 06-25 22:31:38 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-shot-em/checkpoints/Qwen2.5-Math-1.5B/one_shot_1.5b_t0.5/final...
[1;36m(VllmWorkerProcess pid=1033405)[0;0m INFO 06-25 22:31:38 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-shot-em/checkpoints/Qwen2.5-Math-1.5B/one_shot_1.5b_t0.5/final...
[1;36m(VllmWorkerProcess pid=1033405)[0;0m INFO 06-25 22:31:38 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=1033403)[0;0m INFO 06-25 22:31:38 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 22:31:38 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=1033404)[0;0m INFO 06-25 22:31:38 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 22:31:39 distributed_gpu_executor.py:57] # GPU blocks: 187014, # CPU blocks: 18724
INFO 06-25 22:31:39 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 730.52x
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231] Exception in worker VllmWorkerProcess while processing method initialize_cache: CUDA out of memory. Tried to allocate 1.43 GiB. GPU 3 has a total capacity of 47.41 GiB of which 548.06 MiB is free. Process 986870 has 42.66 GiB memory in use. Including non-PyTorch memory, this process has 4.19 GiB memory in use. Of the allocated memory 3.63 GiB is allocated by PyTorch, and 26.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables), Traceback (most recent call last):
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]   File "/homes/gws/lxh22/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py", line 224, in _run_worker_process
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]     output = executor(*args, **kwargs)
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]   File "/homes/gws/lxh22/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/worker/worker.py", line 270, in initialize_cache
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]     self._init_cache_engine()
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]   File "/homes/gws/lxh22/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/worker/worker.py", line 275, in _init_cache_engine
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]     self.cache_engine = [
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]   File "/homes/gws/lxh22/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/worker/worker.py", line 276, in <listcomp>
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]     CacheEngine(self.cache_config, self.model_config,
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]   File "/homes/gws/lxh22/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/worker/cache_engine.py", line 63, in __init__
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]     self.gpu_cache = self._allocate_kv_cache(
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]   File "/homes/gws/lxh22/miniconda3/envs/eval/lib/python3.10/site-packages/vllm/worker/cache_engine.py", line 82, in _allocate_kv_cache
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231]     torch.zeros(kv_cache_shape,
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.43 GiB. GPU 3 has a total capacity of 47.41 GiB of which 548.06 MiB is free. Process 986870 has 42.66 GiB memory in use. Including non-PyTorch memory, this process has 4.19 GiB memory in use. Of the allocated memory 3.63 GiB is allocated by PyTorch, and 26.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorkerProcess pid=1033405)[0;0m ERROR 06-25 22:31:39 multiproc_worker_utils.py:231] 
[1;36m(VllmWorkerProcess pid=1033403)[0;0m INFO 06-25 22:31:43 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=1033403)[0;0m INFO 06-25 22:31:43 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 22:31:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 22:31:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=1033404)[0;0m INFO 06-25 22:31:44 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=1033404)[0;0m INFO 06-25 22:31:44 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
