Random seed set as 0
INFO 06-25 11:11:40 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:11:40 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-100', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-100, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:11:41 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:11:41 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=800108)[0;0m INFO 06-25 11:11:41 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=800106)[0;0m INFO 06-25 11:11:41 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=800107)[0;0m INFO 06-25 11:11:41 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=800108)[0;0m INFO 06-25 11:11:44 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:11:44 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=800108)[0;0m INFO 06-25 11:11:44 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:11:44 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=800106)[0;0m INFO 06-25 11:11:44 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=800107)[0;0m INFO 06-25 11:11:44 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=800106)[0;0m INFO 06-25 11:11:44 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=800107)[0;0m INFO 06-25 11:11:44 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:11:45 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=800106)[0;0m WARNING 06-25 11:11:45 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=800108)[0;0m WARNING 06-25 11:11:45 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=800107)[0;0m WARNING 06-25 11:11:45 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:11:45 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f663baa3580>, local_subscribe_port=52053, remote_subscribe_port=None)
INFO 06-25 11:11:45 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-100...
[1;36m(VllmWorkerProcess pid=800108)[0;0m INFO 06-25 11:11:45 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-100...
[1;36m(VllmWorkerProcess pid=800107)[0;0m INFO 06-25 11:11:45 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-100...
[1;36m(VllmWorkerProcess pid=800106)[0;0m INFO 06-25 11:11:45 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-100...
INFO 06-25 11:11:45 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=800106)[0;0m INFO 06-25 11:11:45 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=800108)[0;0m INFO 06-25 11:11:45 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=800107)[0;0m INFO 06-25 11:11:45 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:11:46 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:11:46 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=800107)[0;0m INFO 06-25 11:11:50 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=800107)[0;0m INFO 06-25 11:11:50 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=800106)[0;0m INFO 06-25 11:11:50 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=800106)[0;0m INFO 06-25 11:11:50 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:11:50 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=800108)[0;0m INFO 06-25 11:11:50 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:11:50 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=800108)[0;0m INFO 06-25 11:11:50 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:12:04 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=800107)[0;0m INFO 06-25 11:12:04 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=800106)[0;0m INFO 06-25 11:12:04 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=800108)[0;0m INFO 06-25 11:12:04 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 39.7, 'total_acc': 39.6875}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-100/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
39.7   	39.7   
INFO 06-25 11:13:20 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=800106)[0;0m INFO 06-25 11:13:20 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=800108)[0;0m INFO 06-25 11:13:20 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=800107)[0;0m INFO 06-25 11:13:20 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:13:49 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:13:49 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1000', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1000', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1000, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:13:50 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:13:50 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=800842)[0;0m INFO 06-25 11:13:50 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=800844)[0;0m INFO 06-25 11:13:50 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=800843)[0;0m INFO 06-25 11:13:50 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:13:53 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:13:53 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=800842)[0;0m INFO 06-25 11:13:53 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=800844)[0;0m INFO 06-25 11:13:53 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=800843)[0;0m INFO 06-25 11:13:53 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=800842)[0;0m INFO 06-25 11:13:53 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=800844)[0;0m INFO 06-25 11:13:53 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=800843)[0;0m INFO 06-25 11:13:53 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:13:53 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=800844)[0;0m WARNING 06-25 11:13:53 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=800842)[0;0m WARNING 06-25 11:13:53 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=800843)[0;0m WARNING 06-25 11:13:53 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:13:53 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f15274173d0>, local_subscribe_port=45309, remote_subscribe_port=None)
INFO 06-25 11:13:53 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1000...
[1;36m(VllmWorkerProcess pid=800842)[0;0m INFO 06-25 11:13:53 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1000...
[1;36m(VllmWorkerProcess pid=800844)[0;0m INFO 06-25 11:13:53 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1000...
[1;36m(VllmWorkerProcess pid=800843)[0;0m INFO 06-25 11:13:53 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1000...
INFO 06-25 11:13:54 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=800843)[0;0m INFO 06-25 11:13:54 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=800844)[0;0m INFO 06-25 11:13:54 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=800842)[0;0m INFO 06-25 11:13:54 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:13:54 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:13:54 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=800843)[0;0m INFO 06-25 11:13:59 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=800843)[0;0m INFO 06-25 11:13:59 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=800842)[0;0m INFO 06-25 11:13:59 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=800842)[0;0m INFO 06-25 11:13:59 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:13:59 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:13:59 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=800844)[0;0m INFO 06-25 11:13:59 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=800844)[0;0m INFO 06-25 11:13:59 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=800842)[0;0m INFO 06-25 11:14:17 model_runner.py:1530] Graph capturing finished in 18 secs.
INFO 06-25 11:14:17 model_runner.py:1530] Graph capturing finished in 18 secs.
[1;36m(VllmWorkerProcess pid=800843)[0;0m INFO 06-25 11:14:17 model_runner.py:1530] Graph capturing finished in 18 secs.
[1;36m(VllmWorkerProcess pid=800844)[0;0m INFO 06-25 11:14:17 model_runner.py:1530] Graph capturing finished in 18 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 5, 'acc': 34.7, 'total_acc': 34.6875}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1000/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
34.7   	34.7   
INFO 06-25 11:15:19 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=800842)[0;0m INFO 06-25 11:15:19 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=800844)[0;0m [1;36m(VllmWorkerProcess pid=800843)[0;0m INFO 06-25 11:15:19 multiproc_worker_utils.py:242] Worker exiting
INFO 06-25 11:15:19 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:15:44 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:15:44 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1100', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1100, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:15:45 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:15:45 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=801751)[0;0m INFO 06-25 11:15:45 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=801750)[0;0m INFO 06-25 11:15:45 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=801752)[0;0m INFO 06-25 11:15:45 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:15:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=801751)[0;0m INFO 06-25 11:15:48 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:15:48 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=801751)[0;0m INFO 06-25 11:15:48 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=801750)[0;0m INFO 06-25 11:15:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=801752)[0;0m INFO 06-25 11:15:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=801750)[0;0m INFO 06-25 11:15:48 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=801752)[0;0m INFO 06-25 11:15:48 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=801751)[0;0m WARNING 06-25 11:15:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:15:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=801752)[0;0m [1;36m(VllmWorkerProcess pid=801750)[0;0m WARNING 06-25 11:15:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:15:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:15:49 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fe8e540efe0>, local_subscribe_port=55737, remote_subscribe_port=None)
INFO 06-25 11:15:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1100...
[1;36m(VllmWorkerProcess pid=801750)[0;0m [1;36m(VllmWorkerProcess pid=801751)[0;0m [1;36m(VllmWorkerProcess pid=801752)[0;0m INFO 06-25 11:15:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1100...
INFO 06-25 11:15:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1100...
INFO 06-25 11:15:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1100...
INFO 06-25 11:15:49 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=801751)[0;0m INFO 06-25 11:15:49 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=801750)[0;0m INFO 06-25 11:15:49 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=801752)[0;0m INFO 06-25 11:15:49 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:15:50 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:15:50 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=801750)[0;0m INFO 06-25 11:15:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=801750)[0;0m INFO 06-25 11:15:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:15:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=801751)[0;0m INFO 06-25 11:15:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:15:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=801752)[0;0m [1;36m(VllmWorkerProcess pid=801751)[0;0m INFO 06-25 11:15:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:15:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=801752)[0;0m INFO 06-25 11:15:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:16:08 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=801752)[0;0m INFO 06-25 11:16:08 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=801751)[0;0m INFO 06-25 11:16:08 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=801750)[0;0m INFO 06-25 11:16:08 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 4, 'acc': 35.9, 'total_acc': 35.9375}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1100/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
35.9   	35.9   
INFO 06-25 11:17:13 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=801751)[0;0m INFO 06-25 11:17:13 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=801750)[0;0m INFO 06-25 11:17:13 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=801752)[0;0m INFO 06-25 11:17:13 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:17:45 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:17:45 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1200', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1200, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:17:45 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:17:45 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=802688)[0;0m INFO 06-25 11:17:45 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=802687)[0;0m INFO 06-25 11:17:45 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=802686)[0;0m INFO 06-25 11:17:45 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:17:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=802687)[0;0m INFO 06-25 11:17:48 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:17:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=802688)[0;0m INFO 06-25 11:17:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=802686)[0;0m INFO 06-25 11:17:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=802687)[0;0m INFO 06-25 11:17:48 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=802688)[0;0m INFO 06-25 11:17:48 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=802686)[0;0m INFO 06-25 11:17:48 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:17:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=802688)[0;0m WARNING 06-25 11:17:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=802687)[0;0m WARNING 06-25 11:17:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=802686)[0;0m WARNING 06-25 11:17:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:17:49 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f16c508f3d0>, local_subscribe_port=34353, remote_subscribe_port=None)
INFO 06-25 11:17:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1200...
[1;36m(VllmWorkerProcess pid=802686)[0;0m INFO 06-25 11:17:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1200...
[1;36m(VllmWorkerProcess pid=802688)[0;0m [1;36m(VllmWorkerProcess pid=802687)[0;0m INFO 06-25 11:17:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1200...
INFO 06-25 11:17:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1200...
INFO 06-25 11:17:49 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=802687)[0;0m INFO 06-25 11:17:49 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=802686)[0;0m INFO 06-25 11:17:49 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=802688)[0;0m INFO 06-25 11:17:49 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:17:50 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:17:50 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=802686)[0;0m INFO 06-25 11:17:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=802686)[0;0m INFO 06-25 11:17:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:17:55 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=802688)[0;0m INFO 06-25 11:17:55 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:17:55 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=802688)[0;0m INFO 06-25 11:17:55 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=802687)[0;0m INFO 06-25 11:17:55 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=802687)[0;0m INFO 06-25 11:17:55 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:18:09 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=802687)[0;0m INFO 06-25 11:18:09 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=802688)[0;0m INFO 06-25 11:18:09 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=802686)[0;0m INFO 06-25 11:18:09 model_runner.py:1530] Graph capturing finished in 15 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 6, 'acc': 37.8, 'total_acc': 37.8125}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1200/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
37.8   	37.8   
INFO 06-25 11:19:10 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=802688)[0;0m INFO 06-25 11:19:10 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=802686)[0;0m INFO 06-25 11:19:10 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=802687)[0;0m INFO 06-25 11:19:10 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:19:42 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:19:42 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1300', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1300, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:19:42 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:19:42 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=803600)[0;0m INFO 06-25 11:19:43 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=803599)[0;0m INFO 06-25 11:19:43 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=803598)[0;0m INFO 06-25 11:19:43 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=803600)[0;0m INFO 06-25 11:19:46 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:19:46 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=803600)[0;0m INFO 06-25 11:19:46 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=803599)[0;0m INFO 06-25 11:19:46 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:19:46 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=803598)[0;0m INFO 06-25 11:19:46 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=803599)[0;0m INFO 06-25 11:19:46 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=803598)[0;0m INFO 06-25 11:19:46 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=803599)[0;0m WARNING 06-25 11:19:46 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=803600)[0;0m WARNING 06-25 11:19:46 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:19:46 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=803598)[0;0m WARNING 06-25 11:19:46 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:19:46 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f6a474cef80>, local_subscribe_port=47799, remote_subscribe_port=None)
INFO 06-25 11:19:46 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1300...
[1;36m(VllmWorkerProcess pid=803599)[0;0m INFO 06-25 11:19:46 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1300...
[1;36m(VllmWorkerProcess pid=803600)[0;0m [1;36m(VllmWorkerProcess pid=803598)[0;0m INFO 06-25 11:19:46 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1300...
INFO 06-25 11:19:46 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1300...
INFO 06-25 11:19:46 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=803600)[0;0m INFO 06-25 11:19:46 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=803599)[0;0m INFO 06-25 11:19:46 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=803598)[0;0m INFO 06-25 11:19:46 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:19:47 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:19:47 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=803598)[0;0m INFO 06-25 11:19:51 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=803598)[0;0m INFO 06-25 11:19:51 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=803599)[0;0m INFO 06-25 11:19:52 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=803599)[0;0m INFO 06-25 11:19:52 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:19:52 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=803600)[0;0m INFO 06-25 11:19:52 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:19:52 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=803600)[0;0m INFO 06-25 11:19:52 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=803599)[0;0m INFO 06-25 11:20:05 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=803600)[0;0m INFO 06-25 11:20:05 model_runner.py:1530] Graph capturing finished in 14 secs.
INFO 06-25 11:20:05 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=803598)[0;0m INFO 06-25 11:20:05 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 5, 'acc': 35.9, 'total_acc': 35.9375}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1300/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
35.9   	35.9   
INFO 06-25 11:21:09 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=803599)[0;0m INFO 06-25 11:21:09 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=803600)[0;0m INFO 06-25 11:21:09 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=803598)[0;0m INFO 06-25 11:21:09 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:21:37 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:21:37 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1400', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1400', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1400, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:21:37 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:21:37 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=804508)[0;0m INFO 06-25 11:21:38 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=804509)[0;0m INFO 06-25 11:21:38 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=804510)[0;0m INFO 06-25 11:21:38 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:21:41 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=804509)[0;0m INFO 06-25 11:21:41 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:21:41 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=804508)[0;0m INFO 06-25 11:21:41 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=804510)[0;0m INFO 06-25 11:21:41 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=804509)[0;0m INFO 06-25 11:21:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=804508)[0;0m INFO 06-25 11:21:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=804510)[0;0m INFO 06-25 11:21:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=804509)[0;0m WARNING 06-25 11:21:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=804508)[0;0m WARNING 06-25 11:21:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:21:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=804510)[0;0m WARNING 06-25 11:21:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:21:41 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fe491092bc0>, local_subscribe_port=57543, remote_subscribe_port=None)
INFO 06-25 11:21:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1400...
[1;36m(VllmWorkerProcess pid=804508)[0;0m INFO 06-25 11:21:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1400...
[1;36m(VllmWorkerProcess pid=804510)[0;0m [1;36m(VllmWorkerProcess pid=804509)[0;0m INFO 06-25 11:21:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1400...
INFO 06-25 11:21:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1400...
[1;36m(VllmWorkerProcess pid=804509)[0;0m INFO 06-25 11:21:41 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:21:41 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=804508)[0;0m INFO 06-25 11:21:41 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=804510)[0;0m INFO 06-25 11:21:41 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:21:42 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:21:42 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=804510)[0;0m INFO 06-25 11:21:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=804510)[0;0m INFO 06-25 11:21:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=804508)[0;0m INFO 06-25 11:21:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=804508)[0;0m INFO 06-25 11:21:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=804509)[0;0m INFO 06-25 11:21:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:21:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=804509)[0;0m INFO 06-25 11:21:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:21:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:22:04 model_runner.py:1530] Graph capturing finished in 17 secs.
[1;36m(VllmWorkerProcess pid=804509)[0;0m INFO 06-25 11:22:04 model_runner.py:1530] Graph capturing finished in 17 secs.
[1;36m(VllmWorkerProcess pid=804508)[0;0m INFO 06-25 11:22:04 model_runner.py:1530] Graph capturing finished in 17 secs.
[1;36m(VllmWorkerProcess pid=804510)[0;0m INFO 06-25 11:22:04 model_runner.py:1530] Graph capturing finished in 17 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 4, 'acc': 38.1, 'total_acc': 38.125}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1400/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
38.1   	38.1   
INFO 06-25 11:23:07 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=804508)[0;0m INFO 06-25 11:23:07 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=804509)[0;0m INFO 06-25 11:23:07 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=804510)[0;0m INFO 06-25 11:23:07 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:23:39 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:23:39 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1500', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1500', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1500, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:23:39 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:23:39 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=805427)[0;0m INFO 06-25 11:23:40 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=805429)[0;0m INFO 06-25 11:23:40 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=805428)[0;0m INFO 06-25 11:23:40 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=805429)[0;0m INFO 06-25 11:23:43 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=805427)[0;0m INFO 06-25 11:23:43 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:23:43 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=805429)[0;0m INFO 06-25 11:23:43 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=805427)[0;0m INFO 06-25 11:23:43 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=805428)[0;0m INFO 06-25 11:23:43 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:23:43 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=805428)[0;0m INFO 06-25 11:23:43 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:23:43 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=805429)[0;0m WARNING 06-25 11:23:43 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=805427)[0;0m WARNING 06-25 11:23:43 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=805428)[0;0m WARNING 06-25 11:23:43 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:23:43 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f3a0ee0b0a0>, local_subscribe_port=42479, remote_subscribe_port=None)
INFO 06-25 11:23:43 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1500...
[1;36m(VllmWorkerProcess pid=805427)[0;0m INFO 06-25 11:23:43 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1500...
[1;36m(VllmWorkerProcess pid=805429)[0;0m INFO 06-25 11:23:43 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1500...
[1;36m(VllmWorkerProcess pid=805428)[0;0m INFO 06-25 11:23:43 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1500...
[1;36m(VllmWorkerProcess pid=805428)[0;0m INFO 06-25 11:23:44 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:23:44 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=805427)[0;0m INFO 06-25 11:23:44 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=805429)[0;0m INFO 06-25 11:23:44 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:23:45 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:23:45 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=805428)[0;0m INFO 06-25 11:23:49 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=805428)[0;0m INFO 06-25 11:23:49 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:23:49 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:23:49 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=805429)[0;0m [1;36m(VllmWorkerProcess pid=805427)[0;0m INFO 06-25 11:23:49 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:23:49 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=805429)[0;0m [1;36m(VllmWorkerProcess pid=805427)[0;0m INFO 06-25 11:23:49 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:23:49 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=805427)[0;0m INFO 06-25 11:24:02 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=805429)[0;0m INFO 06-25 11:24:02 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=805428)[0;0m INFO 06-25 11:24:02 model_runner.py:1530] Graph capturing finished in 13 secs.
INFO 06-25 11:24:02 model_runner.py:1530] Graph capturing finished in 13 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 7, 'acc': 34.1, 'total_acc': 34.0625}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1500/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
34.1   	34.1   
INFO 06-25 11:25:05 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=805427)[0;0m INFO 06-25 11:25:05 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=805429)[0;0m INFO 06-25 11:25:05 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=805428)[0;0m INFO 06-25 11:25:05 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:25:36 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:25:36 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1600', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1600', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1600, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:25:37 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:25:37 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=806850)[0;0m INFO 06-25 11:25:37 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=806852)[0;0m INFO 06-25 11:25:37 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=806851)[0;0m INFO 06-25 11:25:37 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=806850)[0;0m INFO 06-25 11:25:40 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=806852)[0;0m INFO 06-25 11:25:40 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:25:40 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=806850)[0;0m INFO 06-25 11:25:40 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=806852)[0;0m INFO 06-25 11:25:40 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:25:40 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=806851)[0;0m INFO 06-25 11:25:40 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=806851)[0;0m INFO 06-25 11:25:40 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:25:40 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=806850)[0;0m WARNING 06-25 11:25:40 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=806852)[0;0m WARNING 06-25 11:25:40 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=806851)[0;0m WARNING 06-25 11:25:40 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:25:40 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fcbcf20b070>, local_subscribe_port=46215, remote_subscribe_port=None)
INFO 06-25 11:25:40 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1600...
[1;36m(VllmWorkerProcess pid=806850)[0;0m INFO 06-25 11:25:40 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1600...
[1;36m(VllmWorkerProcess pid=806852)[0;0m INFO 06-25 11:25:40 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1600...
[1;36m(VllmWorkerProcess pid=806851)[0;0m INFO 06-25 11:25:40 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1600...
[1;36m(VllmWorkerProcess pid=806852)[0;0m INFO 06-25 11:25:41 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:25:41 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=806851)[0;0m INFO 06-25 11:25:41 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=806850)[0;0m INFO 06-25 11:25:41 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:25:42 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:25:42 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=806851)[0;0m INFO 06-25 11:25:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=806851)[0;0m INFO 06-25 11:25:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:25:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:25:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=806852)[0;0m [1;36m(VllmWorkerProcess pid=806850)[0;0m INFO 06-25 11:25:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:25:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=806852)[0;0m [1;36m(VllmWorkerProcess pid=806850)[0;0m INFO 06-25 11:25:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:25:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:26:00 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=806850)[0;0m INFO 06-25 11:26:00 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=806851)[0;0m INFO 06-25 11:26:00 model_runner.py:1530] Graph capturing finished in 15 secs.
[1;36m(VllmWorkerProcess pid=806852)[0;0m INFO 06-25 11:26:00 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
-------------------- Epoch 3
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 6, 'acc': 39.4, 'total_acc': 39.375}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1600/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
39.4   	39.4   
INFO 06-25 11:27:08 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=806850)[0;0m INFO 06-25 11:27:08 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=806852)[0;0m INFO 06-25 11:27:08 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=806851)[0;0m INFO 06-25 11:27:08 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:27:37 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:27:37 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1700', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1700', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1700, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:27:38 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:27:38 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=807994)[0;0m INFO 06-25 11:27:38 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=807996)[0;0m [1;36m(VllmWorkerProcess pid=807995)[0;0m INFO 06-25 11:27:38 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:27:38 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:27:41 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=807994)[0;0m INFO 06-25 11:27:41 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:27:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=807994)[0;0m INFO 06-25 11:27:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=807996)[0;0m [1;36m(VllmWorkerProcess pid=807995)[0;0m INFO 06-25 11:27:41 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:27:41 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=807996)[0;0m [1;36m(VllmWorkerProcess pid=807995)[0;0m INFO 06-25 11:27:41 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:27:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=807994)[0;0m WARNING 06-25 11:27:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:27:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=807996)[0;0m WARNING 06-25 11:27:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=807995)[0;0m WARNING 06-25 11:27:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:27:41 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fc1b88a31f0>, local_subscribe_port=59259, remote_subscribe_port=None)
INFO 06-25 11:27:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1700...
[1;36m(VllmWorkerProcess pid=807994)[0;0m INFO 06-25 11:27:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1700...
[1;36m(VllmWorkerProcess pid=807996)[0;0m INFO 06-25 11:27:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1700...
[1;36m(VllmWorkerProcess pid=807995)[0;0m INFO 06-25 11:27:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1700...
INFO 06-25 11:27:42 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=807994)[0;0m INFO 06-25 11:27:42 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=807996)[0;0m [1;36m(VllmWorkerProcess pid=807995)[0;0m INFO 06-25 11:27:42 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:27:42 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:27:43 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:27:43 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=807994)[0;0m INFO 06-25 11:27:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=807994)[0;0m INFO 06-25 11:27:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:27:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=807995)[0;0m INFO 06-25 11:27:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:27:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=807996)[0;0m [1;36m(VllmWorkerProcess pid=807995)[0;0m INFO 06-25 11:27:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:27:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=807996)[0;0m INFO 06-25 11:27:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:28:01 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=807994)[0;0m INFO 06-25 11:28:01 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=807996)[0;0m INFO 06-25 11:28:01 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=807995)[0;0m INFO 06-25 11:28:01 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 7, 'acc': 35.0, 'total_acc': 35.0}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1700/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
35.0   	35.0   
INFO 06-25 11:29:00 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=807994)[0;0m INFO 06-25 11:29:00 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=807996)[0;0m [1;36m(VllmWorkerProcess pid=807995)[0;0m INFO 06-25 11:29:00 multiproc_worker_utils.py:242] Worker exiting
INFO 06-25 11:29:00 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:29:32 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:29:32 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1800', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1800', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1800, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:29:32 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:29:32 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=808892)[0;0m INFO 06-25 11:29:32 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=808891)[0;0m INFO 06-25 11:29:33 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=808893)[0;0m INFO 06-25 11:29:33 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:29:41 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:29:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=808892)[0;0m INFO 06-25 11:29:41 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=808891)[0;0m INFO 06-25 11:29:41 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=808893)[0;0m [1;36m(VllmWorkerProcess pid=808892)[0;0m INFO 06-25 11:29:41 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:29:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=808891)[0;0m INFO 06-25 11:29:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=808893)[0;0m INFO 06-25 11:29:41 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=808893)[0;0m WARNING 06-25 11:29:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:29:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=808892)[0;0m WARNING 06-25 11:29:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=808891)[0;0m WARNING 06-25 11:29:41 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:29:41 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f55cd87b070>, local_subscribe_port=39323, remote_subscribe_port=None)
INFO 06-25 11:29:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1800...
[1;36m(VllmWorkerProcess pid=808892)[0;0m [1;36m(VllmWorkerProcess pid=808891)[0;0m INFO 06-25 11:29:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1800...
INFO 06-25 11:29:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1800...
[1;36m(VllmWorkerProcess pid=808893)[0;0m INFO 06-25 11:29:41 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1800...
INFO 06-25 11:29:41 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=808892)[0;0m INFO 06-25 11:29:41 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=808893)[0;0m INFO 06-25 11:29:42 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=808891)[0;0m INFO 06-25 11:29:42 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:29:42 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:29:42 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=808893)[0;0m INFO 06-25 11:29:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=808893)[0;0m [1;36m(VllmWorkerProcess pid=808891)[0;0m INFO 06-25 11:29:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:29:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=808891)[0;0m INFO 06-25 11:29:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:29:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:29:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=808892)[0;0m INFO 06-25 11:29:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=808892)[0;0m INFO 06-25 11:29:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=808892)[0;0m INFO 06-25 11:30:00 model_runner.py:1530] Graph capturing finished in 14 secs.
INFO 06-25 11:30:00 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=808893)[0;0m INFO 06-25 11:30:00 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=808891)[0;0m INFO 06-25 11:30:00 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 11, 'acc': 35.6, 'total_acc': 35.625}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1800/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
35.6   	35.6   
INFO 06-25 11:30:58 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=808891)[0;0m INFO 06-25 11:30:58 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=808892)[0;0m INFO 06-25 11:30:58 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=808893)[0;0m INFO 06-25 11:30:58 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:31:24 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:31:24 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1900', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1900', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1900, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:31:24 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:31:24 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=809980)[0;0m INFO 06-25 11:31:25 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=809981)[0;0m INFO 06-25 11:31:25 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=809979)[0;0m INFO 06-25 11:31:25 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:31:27 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=809980)[0;0m INFO 06-25 11:31:27 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=809981)[0;0m INFO 06-25 11:31:27 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:31:27 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=809979)[0;0m INFO 06-25 11:31:27 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=809980)[0;0m INFO 06-25 11:31:27 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=809981)[0;0m INFO 06-25 11:31:27 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=809979)[0;0m INFO 06-25 11:31:27 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=809980)[0;0m [1;36m(VllmWorkerProcess pid=809981)[0;0m WARNING 06-25 11:31:28 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:31:28 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:31:28 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=809979)[0;0m WARNING 06-25 11:31:28 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:31:28 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f2c0cc973a0>, local_subscribe_port=36703, remote_subscribe_port=None)
INFO 06-25 11:31:28 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1900...
[1;36m(VllmWorkerProcess pid=809979)[0;0m INFO 06-25 11:31:28 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1900...
[1;36m(VllmWorkerProcess pid=809980)[0;0m [1;36m(VllmWorkerProcess pid=809981)[0;0m INFO 06-25 11:31:28 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1900...
INFO 06-25 11:31:28 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1900...
INFO 06-25 11:31:28 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=809980)[0;0m INFO 06-25 11:31:28 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=809981)[0;0m INFO 06-25 11:31:28 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=809979)[0;0m INFO 06-25 11:31:28 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:31:29 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:31:29 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=809979)[0;0m INFO 06-25 11:31:33 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=809979)[0;0m INFO 06-25 11:31:33 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:31:34 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:31:34 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=809980)[0;0m [1;36m(VllmWorkerProcess pid=809981)[0;0m INFO 06-25 11:31:34 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:31:34 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=809980)[0;0m INFO 06-25 11:31:34 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=809981)[0;0m INFO 06-25 11:31:34 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=809981)[0;0m INFO 06-25 11:31:47 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=809980)[0;0m INFO 06-25 11:31:47 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=809979)[0;0m INFO 06-25 11:31:47 model_runner.py:1530] Graph capturing finished in 14 secs.
INFO 06-25 11:31:47 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 5, 'acc': 38.8, 'total_acc': 38.75}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-1900/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
38.8   	38.8   
INFO 06-25 11:32:47 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=809981)[0;0m INFO 06-25 11:32:47 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=809980)[0;0m INFO 06-25 11:32:47 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=809979)[0;0m INFO 06-25 11:32:47 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:33:17 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:33:17 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-200', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-200, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:33:17 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:33:17 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=810921)[0;0m [1;36m(VllmWorkerProcess pid=810919)[0;0m INFO 06-25 11:33:18 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:33:18 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=810920)[0;0m INFO 06-25 11:33:18 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:33:21 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=810919)[0;0m INFO 06-25 11:33:21 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:33:21 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=810921)[0;0m [1;36m(VllmWorkerProcess pid=810920)[0;0m INFO 06-25 11:33:21 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:33:21 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=810919)[0;0m INFO 06-25 11:33:21 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=810921)[0;0m INFO 06-25 11:33:21 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=810920)[0;0m INFO 06-25 11:33:21 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:33:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=810921)[0;0m WARNING 06-25 11:33:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=810919)[0;0m [1;36m(VllmWorkerProcess pid=810920)[0;0m WARNING 06-25 11:33:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:33:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:33:21 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f52e1e032e0>, local_subscribe_port=56239, remote_subscribe_port=None)
INFO 06-25 11:33:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-200...
[1;36m(VllmWorkerProcess pid=810919)[0;0m [1;36m(VllmWorkerProcess pid=810921)[0;0m INFO 06-25 11:33:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-200...
INFO 06-25 11:33:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-200...
[1;36m(VllmWorkerProcess pid=810920)[0;0m INFO 06-25 11:33:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-200...
INFO 06-25 11:33:21 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=810921)[0;0m INFO 06-25 11:33:21 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=810919)[0;0m INFO 06-25 11:33:21 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=810920)[0;0m INFO 06-25 11:33:22 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:33:22 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:33:22 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=810920)[0;0m INFO 06-25 11:33:26 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=810920)[0;0m INFO 06-25 11:33:26 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:33:27 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:33:27 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=810921)[0;0m [1;36m(VllmWorkerProcess pid=810919)[0;0m INFO 06-25 11:33:27 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:33:27 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=810919)[0;0m [1;36m(VllmWorkerProcess pid=810921)[0;0m INFO 06-25 11:33:27 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:33:27 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:33:42 model_runner.py:1530] Graph capturing finished in 15 secs.
[1;36m(VllmWorkerProcess pid=810921)[0;0m INFO 06-25 11:33:42 model_runner.py:1530] Graph capturing finished in 15 secs.
[1;36m(VllmWorkerProcess pid=810920)[0;0m INFO 06-25 11:33:42 model_runner.py:1530] Graph capturing finished in 15 secs.
[1;36m(VllmWorkerProcess pid=810919)[0;0m INFO 06-25 11:33:42 model_runner.py:1530] Graph capturing finished in 15 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 1, 'acc': 39.7, 'total_acc': 39.6875}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-200/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
39.7   	39.7   
INFO 06-25 11:34:46 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=810921)[0;0m INFO 06-25 11:34:46 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=810919)[0;0m INFO 06-25 11:34:46 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=810920)[0;0m INFO 06-25 11:34:46 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:35:17 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:35:17 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2000', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2000', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2000, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:35:17 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:35:17 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=812176)[0;0m INFO 06-25 11:35:17 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=812175)[0;0m INFO 06-25 11:35:17 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=812174)[0;0m INFO 06-25 11:35:17 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=812176)[0;0m INFO 06-25 11:35:20 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:35:20 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=812176)[0;0m INFO 06-25 11:35:20 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=812175)[0;0m INFO 06-25 11:35:20 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=812174)[0;0m INFO 06-25 11:35:20 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:35:20 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=812175)[0;0m INFO 06-25 11:35:20 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=812174)[0;0m INFO 06-25 11:35:20 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:35:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=812176)[0;0m WARNING 06-25 11:35:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=812175)[0;0m WARNING 06-25 11:35:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=812174)[0;0m WARNING 06-25 11:35:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:35:21 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fba67b43250>, local_subscribe_port=42123, remote_subscribe_port=None)
INFO 06-25 11:35:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2000...
[1;36m(VllmWorkerProcess pid=812176)[0;0m INFO 06-25 11:35:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2000...
[1;36m(VllmWorkerProcess pid=812174)[0;0m [1;36m(VllmWorkerProcess pid=812175)[0;0m INFO 06-25 11:35:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2000...
INFO 06-25 11:35:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2000...
INFO 06-25 11:35:21 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=812174)[0;0m INFO 06-25 11:35:21 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=812176)[0;0m INFO 06-25 11:35:21 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=812175)[0;0m INFO 06-25 11:35:21 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:35:22 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:35:22 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=812174)[0;0m INFO 06-25 11:35:26 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=812174)[0;0m INFO 06-25 11:35:26 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:35:26 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:35:26 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=812175)[0;0m [1;36m(VllmWorkerProcess pid=812176)[0;0m INFO 06-25 11:35:26 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:35:26 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=812175)[0;0m INFO 06-25 11:35:26 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=812176)[0;0m INFO 06-25 11:35:26 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:35:40 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=812176)[0;0m INFO 06-25 11:35:40 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=812175)[0;0m INFO 06-25 11:35:40 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=812174)[0;0m INFO 06-25 11:35:40 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
["raise ValueError('Invalid trajectory')"]
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 18, 'acc': 31.9, 'total_acc': 31.874999999999996}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2000/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
31.9   	31.9   
INFO 06-25 11:36:45 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=812175)[0;0m INFO 06-25 11:36:45 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=812176)[0;0m INFO 06-25 11:36:45 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=812174)[0;0m INFO 06-25 11:36:45 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:37:12 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:37:12 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2100', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2100, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:37:12 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:37:12 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=813492)[0;0m INFO 06-25 11:37:13 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=813491)[0;0m INFO 06-25 11:37:13 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=813490)[0;0m INFO 06-25 11:37:13 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=813491)[0;0m INFO 06-25 11:37:16 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:37:16 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=813491)[0;0m INFO 06-25 11:37:16 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=813492)[0;0m INFO 06-25 11:37:16 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=813490)[0;0m INFO 06-25 11:37:16 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=813492)[0;0m INFO 06-25 11:37:16 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:37:16 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=813490)[0;0m INFO 06-25 11:37:16 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:37:16 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=813492)[0;0m WARNING 06-25 11:37:16 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=813491)[0;0m WARNING 06-25 11:37:16 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=813490)[0;0m WARNING 06-25 11:37:16 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:37:16 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f1f57a1f640>, local_subscribe_port=40991, remote_subscribe_port=None)
INFO 06-25 11:37:16 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2100...
[1;36m(VllmWorkerProcess pid=813491)[0;0m INFO 06-25 11:37:16 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2100...
[1;36m(VllmWorkerProcess pid=813492)[0;0m INFO 06-25 11:37:16 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2100...
[1;36m(VllmWorkerProcess pid=813490)[0;0m INFO 06-25 11:37:16 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2100...
[1;36m(VllmWorkerProcess pid=813492)[0;0m INFO 06-25 11:37:17 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=813491)[0;0m INFO 06-25 11:37:17 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=813490)[0;0m INFO 06-25 11:37:17 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:37:17 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:37:18 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:37:18 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=813490)[0;0m INFO 06-25 11:37:22 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=813490)[0;0m INFO 06-25 11:37:22 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=813491)[0;0m INFO 06-25 11:37:22 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=813491)[0;0m INFO 06-25 11:37:22 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:37:22 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:37:22 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=813492)[0;0m INFO 06-25 11:37:22 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=813492)[0;0m INFO 06-25 11:37:22 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=813492)[0;0m INFO 06-25 11:37:36 model_runner.py:1530] Graph capturing finished in 14 secs.
INFO 06-25 11:37:36 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=813491)[0;0m INFO 06-25 11:37:36 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=813490)[0;0m INFO 06-25 11:37:36 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
-------------------- Epoch 3
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 12, 'acc': 32.5, 'total_acc': 32.5}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2100/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
32.5   	32.5   
INFO 06-25 11:38:46 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=813490)[0;0m INFO 06-25 11:38:46 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=813492)[0;0m INFO 06-25 11:38:46 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=813491)[0;0m INFO 06-25 11:38:46 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:39:15 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:39:15 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2200', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2200', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2200, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:39:16 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:39:16 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=814511)[0;0m INFO 06-25 11:39:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=814510)[0;0m [1;36m(VllmWorkerProcess pid=814509)[0;0m INFO 06-25 11:39:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:39:16 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=814511)[0;0m INFO 06-25 11:39:19 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=814509)[0;0m INFO 06-25 11:39:19 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=814511)[0;0m INFO 06-25 11:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=814509)[0;0m INFO 06-25 11:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:39:19 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=814510)[0;0m INFO 06-25 11:39:19 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=814510)[0;0m INFO 06-25 11:39:19 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:39:19 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=814509)[0;0m WARNING 06-25 11:39:19 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=814510)[0;0m WARNING 06-25 11:39:19 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=814511)[0;0m WARNING 06-25 11:39:19 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:39:19 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f4c7909ef20>, local_subscribe_port=50787, remote_subscribe_port=None)
INFO 06-25 11:39:19 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2200...
[1;36m(VllmWorkerProcess pid=814510)[0;0m INFO 06-25 11:39:19 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2200...
[1;36m(VllmWorkerProcess pid=814511)[0;0m [1;36m(VllmWorkerProcess pid=814509)[0;0m INFO 06-25 11:39:19 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2200...
INFO 06-25 11:39:19 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2200...
INFO 06-25 11:39:20 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=814510)[0;0m INFO 06-25 11:39:20 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=814509)[0;0m INFO 06-25 11:39:20 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=814511)[0;0m INFO 06-25 11:39:20 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:39:20 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:39:20 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=814511)[0;0m INFO 06-25 11:39:25 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=814511)[0;0m INFO 06-25 11:39:25 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:39:25 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=814509)[0;0m INFO 06-25 11:39:25 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:39:25 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=814509)[0;0m INFO 06-25 11:39:25 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=814510)[0;0m INFO 06-25 11:39:25 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=814510)[0;0m INFO 06-25 11:39:25 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:39:38 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=814510)[0;0m INFO 06-25 11:39:38 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=814509)[0;0m INFO 06-25 11:39:38 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=814511)[0;0m INFO 06-25 11:39:38 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 13, 'acc': 38.1, 'total_acc': 38.125}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2200/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
38.1   	38.1   
INFO 06-25 11:40:45 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=814509)[0;0m [1;36m(VllmWorkerProcess pid=814511)[0;0m [1;36m(VllmWorkerProcess pid=814510)[0;0m INFO 06-25 11:40:45 multiproc_worker_utils.py:242] Worker exiting
INFO 06-25 11:40:45 multiproc_worker_utils.py:242] Worker exiting
INFO 06-25 11:40:45 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:41:17 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:41:17 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2300', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2300, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:41:17 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:41:17 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=815498)[0;0m INFO 06-25 11:41:17 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=815499)[0;0m INFO 06-25 11:41:17 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=815500)[0;0m INFO 06-25 11:41:17 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:41:21 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=815498)[0;0m INFO 06-25 11:41:21 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:41:21 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=815499)[0;0m [1;36m(VllmWorkerProcess pid=815498)[0;0m INFO 06-25 11:41:21 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=815500)[0;0m INFO 06-25 11:41:21 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:41:21 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=815499)[0;0m INFO 06-25 11:41:21 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=815500)[0;0m INFO 06-25 11:41:21 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=815498)[0;0m WARNING 06-25 11:41:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:41:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=815499)[0;0m WARNING 06-25 11:41:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=815500)[0;0m WARNING 06-25 11:41:21 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:41:21 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f442d493490>, local_subscribe_port=58395, remote_subscribe_port=None)
INFO 06-25 11:41:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2300...
[1;36m(VllmWorkerProcess pid=815498)[0;0m INFO 06-25 11:41:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2300...
[1;36m(VllmWorkerProcess pid=815499)[0;0m INFO 06-25 11:41:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2300...
[1;36m(VllmWorkerProcess pid=815500)[0;0m INFO 06-25 11:41:21 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2300...
[1;36m(VllmWorkerProcess pid=815500)[0;0m INFO 06-25 11:41:22 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:41:22 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=815499)[0;0m INFO 06-25 11:41:22 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=815498)[0;0m INFO 06-25 11:41:22 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:41:23 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:41:23 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=815500)[0;0m INFO 06-25 11:41:27 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=815500)[0;0m INFO 06-25 11:41:27 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:41:27 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:41:27 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=815498)[0;0m INFO 06-25 11:41:27 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=815499)[0;0m INFO 06-25 11:41:27 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=815498)[0;0m INFO 06-25 11:41:27 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=815499)[0;0m INFO 06-25 11:41:27 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:41:41 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=815498)[0;0m INFO 06-25 11:41:41 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=815499)[0;0m INFO 06-25 11:41:41 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=815500)[0;0m INFO 06-25 11:41:41 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 21, 'acc': 35.9, 'total_acc': 35.9375}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2300/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
35.9   	35.9   
INFO 06-25 11:42:38 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=815499)[0;0m [1;36m(VllmWorkerProcess pid=815498)[0;0m INFO 06-25 11:42:38 multiproc_worker_utils.py:242] Worker exiting
INFO 06-25 11:42:38 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=815500)[0;0m INFO 06-25 11:42:38 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:43:08 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:43:08 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2400', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2400', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2400, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:43:09 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:43:09 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=816447)[0;0m INFO 06-25 11:43:09 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=816449)[0;0m INFO 06-25 11:43:09 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=816448)[0;0m INFO 06-25 11:43:09 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=816447)[0;0m INFO 06-25 11:43:12 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:43:12 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=816447)[0;0m INFO 06-25 11:43:12 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:43:12 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=816449)[0;0m [1;36m(VllmWorkerProcess pid=816448)[0;0m INFO 06-25 11:43:12 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:43:12 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=816449)[0;0m INFO 06-25 11:43:12 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=816448)[0;0m INFO 06-25 11:43:12 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:43:12 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=816449)[0;0m WARNING 06-25 11:43:12 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=816447)[0;0m WARNING 06-25 11:43:12 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=816448)[0;0m WARNING 06-25 11:43:12 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:43:12 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f48034cb220>, local_subscribe_port=42697, remote_subscribe_port=None)
INFO 06-25 11:43:12 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2400...
[1;36m(VllmWorkerProcess pid=816447)[0;0m INFO 06-25 11:43:12 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2400...
[1;36m(VllmWorkerProcess pid=816448)[0;0m [1;36m(VllmWorkerProcess pid=816449)[0;0m INFO 06-25 11:43:12 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2400...
INFO 06-25 11:43:12 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2400...
INFO 06-25 11:43:13 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=816449)[0;0m INFO 06-25 11:43:13 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=816447)[0;0m INFO 06-25 11:43:13 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=816448)[0;0m INFO 06-25 11:43:13 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:43:13 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:43:13 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=816448)[0;0m INFO 06-25 11:43:18 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=816448)[0;0m INFO 06-25 11:43:18 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=816447)[0;0m INFO 06-25 11:43:18 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=816447)[0;0m INFO 06-25 11:43:18 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=816449)[0;0m INFO 06-25 11:43:18 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:43:18 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=816449)[0;0m INFO 06-25 11:43:18 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:43:18 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:43:31 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=816447)[0;0m INFO 06-25 11:43:31 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=816449)[0;0m INFO 06-25 11:43:31 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=816448)[0;0m INFO 06-25 11:43:31 model_runner.py:1530] Graph capturing finished in 13 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 22, 'acc': 35.0, 'total_acc': 35.0}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2400/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
35.0   	35.0   
INFO 06-25 11:44:32 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=816447)[0;0m INFO 06-25 11:44:32 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=816449)[0;0m INFO 06-25 11:44:32 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=816448)[0;0m INFO 06-25 11:44:32 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:45:03 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:45:03 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2500', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2500', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2500, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:45:03 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:45:03 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=817421)[0;0m INFO 06-25 11:45:04 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=817419)[0;0m INFO 06-25 11:45:04 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=817420)[0;0m INFO 06-25 11:45:04 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:45:07 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:45:07 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=817419)[0;0m [1;36m(VllmWorkerProcess pid=817421)[0;0m INFO 06-25 11:45:07 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=817420)[0;0m INFO 06-25 11:45:07 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:45:07 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=817419)[0;0m INFO 06-25 11:45:07 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=817421)[0;0m INFO 06-25 11:45:07 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=817420)[0;0m INFO 06-25 11:45:07 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:45:07 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=817421)[0;0m WARNING 06-25 11:45:07 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=817419)[0;0m WARNING 06-25 11:45:07 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=817420)[0;0m WARNING 06-25 11:45:07 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:45:07 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f6e9cbaf1f0>, local_subscribe_port=48831, remote_subscribe_port=None)
INFO 06-25 11:45:07 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2500...
[1;36m(VllmWorkerProcess pid=817419)[0;0m [1;36m(VllmWorkerProcess pid=817421)[0;0m INFO 06-25 11:45:07 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2500...
INFO 06-25 11:45:07 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2500...
[1;36m(VllmWorkerProcess pid=817420)[0;0m INFO 06-25 11:45:07 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2500...
INFO 06-25 11:45:07 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=817421)[0;0m INFO 06-25 11:45:07 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=817419)[0;0m INFO 06-25 11:45:08 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=817420)[0;0m INFO 06-25 11:45:08 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:45:08 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:45:08 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=817420)[0;0m INFO 06-25 11:45:13 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=817420)[0;0m INFO 06-25 11:45:13 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:45:13 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:45:13 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=817421)[0;0m [1;36m(VllmWorkerProcess pid=817419)[0;0m INFO 06-25 11:45:13 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:45:13 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=817421)[0;0m [1;36m(VllmWorkerProcess pid=817419)[0;0m INFO 06-25 11:45:13 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:45:13 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=817419)[0;0m INFO 06-25 11:45:27 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=817421)[0;0m INFO 06-25 11:45:27 model_runner.py:1530] Graph capturing finished in 14 secs.
INFO 06-25 11:45:27 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=817420)[0;0m INFO 06-25 11:45:27 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
['import itertools', 'from fractions import Fraction', 'import itertools', 'from fractions import Fraction', '', '# Define the adjacency list for the icosahedron', 'adjacency_list = {', '    0: [1, 2, 3, 4, 5],', '    1: [0, 6, 7, 8, 9],', '    2: [0, 6, 10, 11, 12],', '    3: [0, 7, 10, 13, 14],', '    4: [0, 8, 11, 13, 15],', '    5: [0, 9, 12, 14, 15],', '    6: [1, 2, 16, 17, 18],', '    7: [1, 3, 16, 19, 20],', '    8: [1, 4, 17, 19, 21],', '    9: [1, 5, 18, 20, 21],', '    10: [2, 3, 16, 22, 23],', '    11: [2, 4, 17, 22, 24],', '    12: [2, 5, 18, 23, 24],', '    13: [3, 4, 19, 22, 25],', '    14: [3, 5, 20, 23, 25],', '    15: [4, 5, 21, 24, 25],', '    16: [6, 7, 10, 13, 14],', '    17: [6, 8, 11, 13, 15],', '    18: [6, 9, 12, 14, 15],', '    19: [7, 8, 10, 16, 17],', '    20: [7, 9, 11, 16, 18],', '    21: [8, 9, 12, 17, 18],', '    22: [10, 11, 13, 16, 19],', '    23: [10, 12, 14, 17, 19],', '    24: [11, 12, 15, 18, 19],', '    25: [13, 14, 15, 20, 21]', '}', '', '# Function to calculate the distance between two vertices', 'def distance(v1, v2):', '    if v1 in adjacency_list[v2]:', '        return 1', '    elif v2 in adjacency_list[v1]:', '        return 1', '    else:', '        return 2', '', '# Count the number of valid configurations', 'valid_count = 0', 'total_count = 0', '', '# Iterate over all possible pairs (Q, R)', 'for Q in range(12):', '    for R in range(12):', '        if Q != R:', '            # Calculate the distance between Q and R', '            d_QR = distance(Q, R)', '            # Iterate over all possible choices for S', '            for S in range(12):', '                if S != Q and S != R:', '                    # Calculate the distance between R and S', '                    d_RS = distance(R, S)', '                    # Check if d(Q, R) > d(R, S)', '                    if d_Q أشهر']
-------------------- Epoch 1
-------------------- Epoch 2
-------------------- Epoch 3
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 17, 'acc': 34.1, 'total_acc': 34.0625}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2500/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
34.1   	34.1   
INFO 06-25 11:46:30 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=817419)[0;0m INFO 06-25 11:46:30 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=817421)[0;0m INFO 06-25 11:46:30 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=817420)[0;0m INFO 06-25 11:46:30 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:47:00 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:47:00 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2600', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2600', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2600, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:47:00 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:47:00 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=818400)[0;0m [1;36m(VllmWorkerProcess pid=818401)[0;0m INFO 06-25 11:47:00 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:47:00 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=818399)[0;0m INFO 06-25 11:47:00 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:47:03 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=818400)[0;0m INFO 06-25 11:47:03 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:47:03 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=818400)[0;0m INFO 06-25 11:47:03 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=818401)[0;0m INFO 06-25 11:47:03 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=818399)[0;0m INFO 06-25 11:47:03 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=818401)[0;0m INFO 06-25 11:47:03 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=818399)[0;0m INFO 06-25 11:47:03 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:47:03 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=818401)[0;0m WARNING 06-25 11:47:03 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=818400)[0;0m [1;36m(VllmWorkerProcess pid=818399)[0;0m WARNING 06-25 11:47:03 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:47:03 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:47:03 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fa769417250>, local_subscribe_port=56229, remote_subscribe_port=None)
INFO 06-25 11:47:04 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2600...
[1;36m(VllmWorkerProcess pid=818399)[0;0m INFO 06-25 11:47:04 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2600...
[1;36m(VllmWorkerProcess pid=818400)[0;0m [1;36m(VllmWorkerProcess pid=818401)[0;0m INFO 06-25 11:47:04 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2600...
INFO 06-25 11:47:04 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2600...
[1;36m(VllmWorkerProcess pid=818399)[0;0m INFO 06-25 11:47:04 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:47:04 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=818400)[0;0m INFO 06-25 11:47:04 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=818401)[0;0m INFO 06-25 11:47:04 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:47:05 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:47:05 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=818399)[0;0m INFO 06-25 11:47:09 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=818399)[0;0m INFO 06-25 11:47:09 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:47:09 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:47:09 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=818400)[0;0m [1;36m(VllmWorkerProcess pid=818401)[0;0m INFO 06-25 11:47:09 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:47:09 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=818400)[0;0m INFO 06-25 11:47:09 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=818401)[0;0m INFO 06-25 11:47:09 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:47:23 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=818401)[0;0m INFO 06-25 11:47:23 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=818400)[0;0m INFO 06-25 11:47:23 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=818399)[0;0m INFO 06-25 11:47:23 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
["raise ValueError('Invalid trajectory')"]
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 17, 'acc': 36.2, 'total_acc': 36.25}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2600/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
36.2   	36.2   
INFO 06-25 11:48:31 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=818400)[0;0m INFO 06-25 11:48:31 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=818399)[0;0m INFO 06-25 11:48:31 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=818401)[0;0m INFO 06-25 11:48:31 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:49:02 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:49:02 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2700', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2700', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2700, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:49:02 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:49:02 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=819371)[0;0m INFO 06-25 11:49:03 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=819373)[0;0m INFO 06-25 11:49:03 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=819372)[0;0m INFO 06-25 11:49:03 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:49:10 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:49:10 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=819371)[0;0m INFO 06-25 11:49:10 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=819373)[0;0m [1;36m(VllmWorkerProcess pid=819372)[0;0m INFO 06-25 11:49:10 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:49:10 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=819373)[0;0m INFO 06-25 11:49:10 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=819371)[0;0m INFO 06-25 11:49:10 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=819372)[0;0m INFO 06-25 11:49:10 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=819371)[0;0m WARNING 06-25 11:49:11 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:49:11 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=819373)[0;0m WARNING 06-25 11:49:11 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=819372)[0;0m WARNING 06-25 11:49:11 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:49:11 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fc4fb092fe0>, local_subscribe_port=51049, remote_subscribe_port=None)
INFO 06-25 11:49:11 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2700...
[1;36m(VllmWorkerProcess pid=819371)[0;0m INFO 06-25 11:49:11 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2700...
[1;36m(VllmWorkerProcess pid=819372)[0;0m [1;36m(VllmWorkerProcess pid=819373)[0;0m INFO 06-25 11:49:11 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2700...
INFO 06-25 11:49:11 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2700...
[1;36m(VllmWorkerProcess pid=819373)[0;0m INFO 06-25 11:49:11 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:49:11 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=819371)[0;0m INFO 06-25 11:49:11 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=819372)[0;0m INFO 06-25 11:49:11 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:49:12 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:49:12 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=819372)[0;0m INFO 06-25 11:49:16 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=819372)[0;0m INFO 06-25 11:49:16 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=819373)[0;0m INFO 06-25 11:49:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:49:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:49:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=819373)[0;0m INFO 06-25 11:49:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=819371)[0;0m INFO 06-25 11:49:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=819371)[0;0m INFO 06-25 11:49:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=819373)[0;0m INFO 06-25 11:49:31 model_runner.py:1530] Graph capturing finished in 14 secs.
INFO 06-25 11:49:31 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=819371)[0;0m INFO 06-25 11:49:31 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=819372)[0;0m INFO 06-25 11:49:31 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 26, 'acc': 30.0, 'total_acc': 30.0}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2700/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
30.0   	30.0   
INFO 06-25 11:50:32 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=819371)[0;0m INFO 06-25 11:50:32 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=819373)[0;0m INFO 06-25 11:50:32 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=819372)[0;0m INFO 06-25 11:50:32 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:51:03 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:51:03 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2800', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2800', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2800, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:51:03 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:51:03 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=820322)[0;0m INFO 06-25 11:51:04 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=820320)[0;0m [1;36m(VllmWorkerProcess pid=820321)[0;0m INFO 06-25 11:51:04 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:51:04 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:51:06 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:51:06 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=820322)[0;0m INFO 06-25 11:51:06 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=820320)[0;0m INFO 06-25 11:51:06 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=820321)[0;0m INFO 06-25 11:51:06 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=820322)[0;0m INFO 06-25 11:51:06 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=820320)[0;0m INFO 06-25 11:51:06 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=820321)[0;0m INFO 06-25 11:51:06 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:51:07 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=820322)[0;0m WARNING 06-25 11:51:07 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=820321)[0;0m [1;36m(VllmWorkerProcess pid=820320)[0;0m WARNING 06-25 11:51:07 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:51:07 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:51:07 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f60cdc83640>, local_subscribe_port=39121, remote_subscribe_port=None)
INFO 06-25 11:51:07 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2800...
[1;36m(VllmWorkerProcess pid=820322)[0;0m INFO 06-25 11:51:07 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2800...
[1;36m(VllmWorkerProcess pid=820320)[0;0m INFO 06-25 11:51:07 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2800...
[1;36m(VllmWorkerProcess pid=820321)[0;0m INFO 06-25 11:51:07 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2800...
INFO 06-25 11:51:07 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=820320)[0;0m INFO 06-25 11:51:07 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=820321)[0;0m INFO 06-25 11:51:07 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=820322)[0;0m INFO 06-25 11:51:07 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:51:08 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:51:08 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=820321)[0;0m INFO 06-25 11:51:12 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=820321)[0;0m INFO 06-25 11:51:12 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=820320)[0;0m INFO 06-25 11:51:12 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=820320)[0;0m INFO 06-25 11:51:12 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:51:12 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:51:12 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=820322)[0;0m INFO 06-25 11:51:12 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=820322)[0;0m INFO 06-25 11:51:12 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:51:26 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=820322)[0;0m INFO 06-25 11:51:26 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=820321)[0;0m INFO 06-25 11:51:26 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=820320)[0;0m INFO 06-25 11:51:26 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
["raise ValueError('Invalid trajectory')"]
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 28, 'acc': 33.1, 'total_acc': 33.125}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2800/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
33.1   	33.1   
INFO 06-25 11:52:20 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=820322)[0;0m [1;36m(VllmWorkerProcess pid=820321)[0;0m INFO 06-25 11:52:20 multiproc_worker_utils.py:242] Worker exiting
INFO 06-25 11:52:20 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=820320)[0;0m INFO 06-25 11:52:20 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:52:45 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:52:45 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2900', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2900', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2900, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:52:46 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:52:46 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=821224)[0;0m INFO 06-25 11:52:46 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=821226)[0;0m INFO 06-25 11:52:46 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=821225)[0;0m INFO 06-25 11:52:46 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:52:49 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:52:49 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=821224)[0;0m [1;36m(VllmWorkerProcess pid=821225)[0;0m INFO 06-25 11:52:49 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=821226)[0;0m INFO 06-25 11:52:49 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:52:49 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=821224)[0;0m [1;36m(VllmWorkerProcess pid=821226)[0;0m INFO 06-25 11:52:49 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:52:49 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=821225)[0;0m INFO 06-25 11:52:49 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=821224)[0;0m WARNING 06-25 11:52:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:52:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=821226)[0;0m WARNING 06-25 11:52:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=821225)[0;0m WARNING 06-25 11:52:49 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:52:49 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fbed32cf190>, local_subscribe_port=42915, remote_subscribe_port=None)
INFO 06-25 11:52:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2900...
[1;36m(VllmWorkerProcess pid=821224)[0;0m [1;36m(VllmWorkerProcess pid=821225)[0;0m INFO 06-25 11:52:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2900...
[1;36m(VllmWorkerProcess pid=821226)[0;0m INFO 06-25 11:52:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2900...
INFO 06-25 11:52:49 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2900...
INFO 06-25 11:52:50 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=821224)[0;0m INFO 06-25 11:52:50 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=821225)[0;0m INFO 06-25 11:52:50 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=821226)[0;0m INFO 06-25 11:52:50 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:52:50 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:52:50 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=821225)[0;0m INFO 06-25 11:52:55 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=821225)[0;0m INFO 06-25 11:52:55 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=821224)[0;0m INFO 06-25 11:52:55 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=821224)[0;0m INFO 06-25 11:52:55 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:52:55 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=821226)[0;0m INFO 06-25 11:52:55 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:52:55 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=821226)[0;0m INFO 06-25 11:52:55 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=821226)[0;0m INFO 06-25 11:53:08 model_runner.py:1530] Graph capturing finished in 13 secs.
INFO 06-25 11:53:08 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=821224)[0;0m INFO 06-25 11:53:08 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=821225)[0;0m INFO 06-25 11:53:08 model_runner.py:1530] Graph capturing finished in 13 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 22, 'acc': 31.2, 'total_acc': 31.25}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-2900/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
31.2   	31.2   
INFO 06-25 11:54:10 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=821224)[0;0m INFO 06-25 11:54:10 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=821225)[0;0m INFO 06-25 11:54:10 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=821226)[0;0m INFO 06-25 11:54:10 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:54:35 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:54:35 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-300', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-300', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-300, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:54:36 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:54:36 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=822169)[0;0m INFO 06-25 11:54:36 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=822168)[0;0m INFO 06-25 11:54:36 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=822167)[0;0m INFO 06-25 11:54:36 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:54:39 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=822167)[0;0m INFO 06-25 11:54:39 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=822168)[0;0m INFO 06-25 11:54:39 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=822169)[0;0m INFO 06-25 11:54:39 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:54:39 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=822167)[0;0m INFO 06-25 11:54:39 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=822168)[0;0m INFO 06-25 11:54:39 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=822169)[0;0m INFO 06-25 11:54:39 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:54:40 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=822167)[0;0m [1;36m(VllmWorkerProcess pid=822169)[0;0m WARNING 06-25 11:54:40 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:54:40 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=822168)[0;0m WARNING 06-25 11:54:40 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:54:40 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f2ed4067340>, local_subscribe_port=43079, remote_subscribe_port=None)
INFO 06-25 11:54:40 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-300...
[1;36m(VllmWorkerProcess pid=822167)[0;0m INFO 06-25 11:54:40 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-300...
[1;36m(VllmWorkerProcess pid=822168)[0;0m [1;36m(VllmWorkerProcess pid=822169)[0;0m INFO 06-25 11:54:40 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-300...
INFO 06-25 11:54:40 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-300...
INFO 06-25 11:54:40 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=822169)[0;0m INFO 06-25 11:54:40 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=822167)[0;0m INFO 06-25 11:54:40 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=822168)[0;0m INFO 06-25 11:54:40 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:54:41 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:54:41 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=822169)[0;0m INFO 06-25 11:54:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=822169)[0;0m INFO 06-25 11:54:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:54:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=822167)[0;0m INFO 06-25 11:54:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=822168)[0;0m INFO 06-25 11:54:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:54:46 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=822167)[0;0m INFO 06-25 11:54:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=822168)[0;0m INFO 06-25 11:54:46 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=822167)[0;0m INFO 06-25 11:55:00 model_runner.py:1530] Graph capturing finished in 14 secs.
INFO 06-25 11:55:00 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=822168)[0;0m INFO 06-25 11:55:00 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=822169)[0;0m INFO 06-25 11:55:00 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 4, 'acc': 37.8, 'total_acc': 37.8125}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-300/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
37.8   	37.8   
INFO 06-25 11:56:00 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=822167)[0;0m INFO 06-25 11:56:00 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=822168)[0;0m INFO 06-25 11:56:00 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=822169)[0;0m INFO 06-25 11:56:00 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:56:26 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:56:26 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-3000', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-3000', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-3000, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:56:26 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:56:26 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=823118)[0;0m INFO 06-25 11:56:27 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=823120)[0;0m INFO 06-25 11:56:27 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=823119)[0;0m INFO 06-25 11:56:27 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=823119)[0;0m INFO 06-25 11:56:30 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:56:30 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=823118)[0;0m [1;36m(VllmWorkerProcess pid=823119)[0;0m INFO 06-25 11:56:30 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:56:30 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=823120)[0;0m INFO 06-25 11:56:30 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:56:30 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=823118)[0;0m [1;36m(VllmWorkerProcess pid=823120)[0;0m INFO 06-25 11:56:30 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 11:56:30 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=823118)[0;0m WARNING 06-25 11:56:30 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:56:30 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=823120)[0;0m [1;36m(VllmWorkerProcess pid=823119)[0;0m WARNING 06-25 11:56:30 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:56:30 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:56:30 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fd36908f310>, local_subscribe_port=35683, remote_subscribe_port=None)
INFO 06-25 11:56:30 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-3000...
[1;36m(VllmWorkerProcess pid=823119)[0;0m [1;36m(VllmWorkerProcess pid=823120)[0;0m INFO 06-25 11:56:30 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-3000...
INFO 06-25 11:56:30 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-3000...
[1;36m(VllmWorkerProcess pid=823118)[0;0m INFO 06-25 11:56:30 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-3000...
[1;36m(VllmWorkerProcess pid=823118)[0;0m INFO 06-25 11:56:30 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:56:30 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=823119)[0;0m INFO 06-25 11:56:31 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=823120)[0;0m INFO 06-25 11:56:31 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:56:31 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:56:31 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=823120)[0;0m INFO 06-25 11:56:36 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:56:36 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=823118)[0;0m INFO 06-25 11:56:36 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=823120)[0;0m INFO 06-25 11:56:36 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:56:36 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=823118)[0;0m INFO 06-25 11:56:36 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=823119)[0;0m INFO 06-25 11:56:36 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=823119)[0;0m INFO 06-25 11:56:36 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:56:49 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=823119)[0;0m INFO 06-25 11:56:49 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=823118)[0;0m INFO 06-25 11:56:49 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=823120)[0;0m INFO 06-25 11:56:49 model_runner.py:1530] Graph capturing finished in 13 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 29, 'acc': 31.6, 'total_acc': 31.5625}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-3000/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
31.6   	31.6   
INFO 06-25 11:57:43 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=823118)[0;0m [1;36m(VllmWorkerProcess pid=823119)[0;0m INFO 06-25 11:57:43 multiproc_worker_utils.py:242] Worker exiting
INFO 06-25 11:57:43 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=823120)[0;0m INFO 06-25 11:57:43 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:58:08 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:58:08 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-400', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-400', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-400, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:58:08 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:58:08 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=824063)[0;0m INFO 06-25 11:58:08 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=824064)[0;0m INFO 06-25 11:58:08 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=824065)[0;0m INFO 06-25 11:58:08 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:58:11 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=824064)[0;0m INFO 06-25 11:58:11 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=824063)[0;0m INFO 06-25 11:58:11 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:58:11 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=824064)[0;0m INFO 06-25 11:58:11 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=824063)[0;0m INFO 06-25 11:58:11 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=824065)[0;0m INFO 06-25 11:58:11 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=824065)[0;0m INFO 06-25 11:58:11 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 11:58:12 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=824063)[0;0m WARNING 06-25 11:58:12 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=824064)[0;0m WARNING 06-25 11:58:12 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=824065)[0;0m WARNING 06-25 11:58:12 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:58:12 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fdb78e1b340>, local_subscribe_port=45613, remote_subscribe_port=None)
INFO 06-25 11:58:12 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-400...
[1;36m(VllmWorkerProcess pid=824063)[0;0m [1;36m(VllmWorkerProcess pid=824064)[0;0m INFO 06-25 11:58:12 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-400...
INFO 06-25 11:58:12 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-400...
[1;36m(VllmWorkerProcess pid=824065)[0;0m INFO 06-25 11:58:12 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-400...
INFO 06-25 11:58:12 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=824064)[0;0m INFO 06-25 11:58:12 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=824065)[0;0m INFO 06-25 11:58:12 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=824063)[0;0m INFO 06-25 11:58:12 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:58:13 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:58:13 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=824065)[0;0m INFO 06-25 11:58:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=824065)[0;0m INFO 06-25 11:58:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:58:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:58:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=824063)[0;0m [1;36m(VllmWorkerProcess pid=824064)[0;0m INFO 06-25 11:58:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 11:58:17 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=824063)[0;0m [1;36m(VllmWorkerProcess pid=824064)[0;0m INFO 06-25 11:58:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 11:58:17 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=824064)[0;0m INFO 06-25 11:58:31 model_runner.py:1530] Graph capturing finished in 14 secs.
INFO 06-25 11:58:31 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=824063)[0;0m INFO 06-25 11:58:31 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=824065)[0;0m INFO 06-25 11:58:31 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 4, 'acc': 40.3, 'total_acc': 40.3125}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-400/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
40.3   	40.3   
INFO 06-25 11:59:32 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=824063)[0;0m INFO 06-25 11:59:32 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=824064)[0;0m INFO 06-25 11:59:32 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=824065)[0;0m INFO 06-25 11:59:32 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 11:59:54 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 11:59:54 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-500', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-500', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-500, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 11:59:54 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 11:59:54 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=825009)[0;0m INFO 06-25 11:59:54 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=825011)[0;0m INFO 06-25 11:59:54 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=825010)[0;0m INFO 06-25 11:59:54 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 11:59:57 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=825009)[0;0m INFO 06-25 11:59:57 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 11:59:57 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=825011)[0;0m INFO 06-25 11:59:57 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=825010)[0;0m INFO 06-25 11:59:57 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=825009)[0;0m INFO 06-25 11:59:57 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=825011)[0;0m INFO 06-25 11:59:57 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=825010)[0;0m INFO 06-25 11:59:57 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=825011)[0;0m WARNING 06-25 11:59:58 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 11:59:58 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=825009)[0;0m WARNING 06-25 11:59:58 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=825010)[0;0m WARNING 06-25 11:59:58 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 11:59:58 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7feed7413250>, local_subscribe_port=59527, remote_subscribe_port=None)
INFO 06-25 11:59:58 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-500...
[1;36m(VllmWorkerProcess pid=825009)[0;0m INFO 06-25 11:59:58 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-500...
[1;36m(VllmWorkerProcess pid=825011)[0;0m INFO 06-25 11:59:58 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-500...
[1;36m(VllmWorkerProcess pid=825010)[0;0m INFO 06-25 11:59:58 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-500...
INFO 06-25 11:59:58 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=825010)[0;0m INFO 06-25 11:59:58 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=825009)[0;0m INFO 06-25 11:59:58 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=825011)[0;0m INFO 06-25 11:59:58 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 11:59:59 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 11:59:59 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=825010)[0;0m INFO 06-25 12:00:03 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=825010)[0;0m INFO 06-25 12:00:03 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=825011)[0;0m INFO 06-25 12:00:03 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=825009)[0;0m [1;36m(VllmWorkerProcess pid=825011)[0;0m INFO 06-25 12:00:03 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 12:00:03 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=825009)[0;0m INFO 06-25 12:00:03 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:00:03 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 12:00:03 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=825009)[0;0m INFO 06-25 12:00:17 model_runner.py:1530] Graph capturing finished in 13 secs.
INFO 06-25 12:00:17 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=825010)[0;0m INFO 06-25 12:00:17 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=825011)[0;0m INFO 06-25 12:00:17 model_runner.py:1530] Graph capturing finished in 13 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 3, 'acc': 39.1, 'total_acc': 39.0625}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-500/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
39.1   	39.1   
INFO 06-25 12:01:19 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=825009)[0;0m INFO 06-25 12:01:19 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=825011)[0;0m INFO 06-25 12:01:19 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=825010)[0;0m INFO 06-25 12:01:19 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 12:01:44 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 12:01:44 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-600', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-600', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-600, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 12:01:45 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 12:01:45 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=826060)[0;0m INFO 06-25 12:01:45 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=826062)[0;0m INFO 06-25 12:01:45 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=826061)[0;0m INFO 06-25 12:01:45 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 12:01:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=826060)[0;0m INFO 06-25 12:01:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=826060)[0;0m INFO 06-25 12:01:48 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 12:01:48 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=826062)[0;0m [1;36m(VllmWorkerProcess pid=826061)[0;0m INFO 06-25 12:01:48 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 12:01:48 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=826062)[0;0m INFO 06-25 12:01:48 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=826061)[0;0m INFO 06-25 12:01:48 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 12:01:48 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=826062)[0;0m WARNING 06-25 12:01:48 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=826060)[0;0m WARNING 06-25 12:01:48 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=826061)[0;0m WARNING 06-25 12:01:48 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 12:01:48 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f6d8e21ad70>, local_subscribe_port=37797, remote_subscribe_port=None)
INFO 06-25 12:01:48 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-600...
[1;36m(VllmWorkerProcess pid=826060)[0;0m INFO 06-25 12:01:48 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-600...
[1;36m(VllmWorkerProcess pid=826061)[0;0m INFO 06-25 12:01:48 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-600...
[1;36m(VllmWorkerProcess pid=826062)[0;0m INFO 06-25 12:01:48 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-600...
[1;36m(VllmWorkerProcess pid=826062)[0;0m INFO 06-25 12:01:49 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 12:01:49 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=826061)[0;0m INFO 06-25 12:01:49 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=826060)[0;0m INFO 06-25 12:01:49 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 12:01:50 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 12:01:50 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=826061)[0;0m INFO 06-25 12:01:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=826061)[0;0m INFO 06-25 12:01:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:01:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 12:01:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=826062)[0;0m INFO 06-25 12:01:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=826060)[0;0m INFO 06-25 12:01:54 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=826062)[0;0m INFO 06-25 12:01:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=826060)[0;0m INFO 06-25 12:01:54 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=826062)[0;0m INFO 06-25 12:02:08 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=826060)[0;0m INFO 06-25 12:02:08 model_runner.py:1530] Graph capturing finished in 14 secs.
INFO 06-25 12:02:08 model_runner.py:1530] Graph capturing finished in 14 secs.
[1;36m(VllmWorkerProcess pid=826061)[0;0m INFO 06-25 12:02:08 model_runner.py:1530] Graph capturing finished in 14 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 3, 'acc': 37.5, 'total_acc': 37.5}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-600/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
37.5   	37.5   
INFO 06-25 12:03:09 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=826060)[0;0m INFO 06-25 12:03:09 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=826061)[0;0m INFO 06-25 12:03:09 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=826062)[0;0m INFO 06-25 12:03:09 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 12:03:41 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 12:03:41 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-700', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-700', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-700, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 12:03:41 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 12:03:41 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=827207)[0;0m INFO 06-25 12:03:42 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=827206)[0;0m INFO 06-25 12:03:42 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=827208)[0;0m INFO 06-25 12:03:42 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 12:03:45 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 12:03:45 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=827206)[0;0m [1;36m(VllmWorkerProcess pid=827207)[0;0m INFO 06-25 12:03:45 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 12:03:45 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=827208)[0;0m INFO 06-25 12:03:45 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=827206)[0;0m [1;36m(VllmWorkerProcess pid=827207)[0;0m INFO 06-25 12:03:45 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 12:03:45 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=827208)[0;0m INFO 06-25 12:03:45 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=827206)[0;0m WARNING 06-25 12:03:45 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 12:03:45 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=827208)[0;0m WARNING 06-25 12:03:45 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=827207)[0;0m WARNING 06-25 12:03:45 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 12:03:45 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ff3600e3220>, local_subscribe_port=57271, remote_subscribe_port=None)
INFO 06-25 12:03:45 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-700...
[1;36m(VllmWorkerProcess pid=827206)[0;0m [1;36m(VllmWorkerProcess pid=827207)[0;0m INFO 06-25 12:03:45 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-700...
INFO 06-25 12:03:45 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-700...
[1;36m(VllmWorkerProcess pid=827208)[0;0m INFO 06-25 12:03:45 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-700...
INFO 06-25 12:03:46 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=827206)[0;0m INFO 06-25 12:03:46 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=827208)[0;0m INFO 06-25 12:03:46 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=827207)[0;0m INFO 06-25 12:03:46 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 12:03:47 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 12:03:47 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=827208)[0;0m INFO 06-25 12:03:51 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=827208)[0;0m INFO 06-25 12:03:51 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=827206)[0;0m INFO 06-25 12:03:51 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=827206)[0;0m INFO 06-25 12:03:51 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=827207)[0;0m INFO 06-25 12:03:51 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 12:03:51 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=827207)[0;0m INFO 06-25 12:03:51 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:03:51 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:04:04 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=827207)[0;0m INFO 06-25 12:04:04 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=827206)[0;0m INFO 06-25 12:04:04 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=827208)[0;0m INFO 06-25 12:04:04 model_runner.py:1530] Graph capturing finished in 13 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 2, 'acc': 39.7, 'total_acc': 39.6875}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-700/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
39.7   	39.7   
INFO 06-25 12:05:08 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=827206)[0;0m INFO 06-25 12:05:08 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=827207)[0;0m INFO 06-25 12:05:08 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=827208)[0;0m INFO 06-25 12:05:08 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 12:05:35 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 12:05:35 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-800', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-800', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-800, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 12:05:35 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 12:05:35 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=828150)[0;0m INFO 06-25 12:05:36 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=828149)[0;0m INFO 06-25 12:05:36 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=828151)[0;0m INFO 06-25 12:05:36 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 12:05:39 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 12:05:39 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=828149)[0;0m [1;36m(VllmWorkerProcess pid=828150)[0;0m INFO 06-25 12:05:39 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 12:05:39 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=828151)[0;0m INFO 06-25 12:05:39 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=828150)[0;0m [1;36m(VllmWorkerProcess pid=828149)[0;0m INFO 06-25 12:05:39 pynccl.py:63] vLLM is using nccl==2.20.5
INFO 06-25 12:05:39 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=828151)[0;0m INFO 06-25 12:05:39 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=828150)[0;0m [1;36m(VllmWorkerProcess pid=828149)[0;0m WARNING 06-25 12:05:39 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 12:05:39 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 12:05:39 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=828151)[0;0m WARNING 06-25 12:05:39 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 12:05:39 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f93b288ae60>, local_subscribe_port=56637, remote_subscribe_port=None)
INFO 06-25 12:05:39 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-800...
[1;36m(VllmWorkerProcess pid=828150)[0;0m INFO 06-25 12:05:39 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-800...
[1;36m(VllmWorkerProcess pid=828149)[0;0m INFO 06-25 12:05:39 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-800...
[1;36m(VllmWorkerProcess pid=828151)[0;0m INFO 06-25 12:05:39 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-800...
[1;36m(VllmWorkerProcess pid=828149)[0;0m INFO 06-25 12:05:39 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 12:05:39 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=828150)[0;0m INFO 06-25 12:05:39 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=828151)[0;0m INFO 06-25 12:05:39 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 12:05:40 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 12:05:40 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=828151)[0;0m INFO 06-25 12:05:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=828151)[0;0m INFO 06-25 12:05:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:05:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=828149)[0;0m INFO 06-25 12:05:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 12:05:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=828149)[0;0m INFO 06-25 12:05:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=828150)[0;0m INFO 06-25 12:05:45 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=828150)[0;0m INFO 06-25 12:05:45 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=828149)[0;0m INFO 06-25 12:06:00 model_runner.py:1530] Graph capturing finished in 15 secs.
INFO 06-25 12:06:00 model_runner.py:1530] Graph capturing finished in 15 secs.
[1;36m(VllmWorkerProcess pid=828150)[0;0m INFO 06-25 12:06:00 model_runner.py:1530] Graph capturing finished in 15 secs.
[1;36m(VllmWorkerProcess pid=828151)[0;0m INFO 06-25 12:06:00 model_runner.py:1530] Graph capturing finished in 15 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
-------------------- Epoch 2
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 5, 'acc': 35.0, 'total_acc': 35.0}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-800/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
35.0   	35.0   
INFO 06-25 12:07:13 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=828149)[0;0m INFO 06-25 12:07:13 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=828150)[0;0m INFO 06-25 12:07:13 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=828151)[0;0m INFO 06-25 12:07:13 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 12:07:43 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 12:07:43 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-900', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-900', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-900, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 12:07:44 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 12:07:44 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=829423)[0;0m INFO 06-25 12:07:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=829425)[0;0m INFO 06-25 12:07:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=829424)[0;0m INFO 06-25 12:07:44 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 12:07:47 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 12:07:47 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=829423)[0;0m INFO 06-25 12:07:47 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=829425)[0;0m INFO 06-25 12:07:47 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=829424)[0;0m INFO 06-25 12:07:47 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=829423)[0;0m INFO 06-25 12:07:47 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=829425)[0;0m INFO 06-25 12:07:47 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=829424)[0;0m INFO 06-25 12:07:47 pynccl.py:63] vLLM is using nccl==2.20.5
WARNING 06-25 12:07:47 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=829423)[0;0m WARNING 06-25 12:07:47 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=829425)[0;0m WARNING 06-25 12:07:47 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=829424)[0;0m WARNING 06-25 12:07:47 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 12:07:47 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f8479c8af80>, local_subscribe_port=41127, remote_subscribe_port=None)
INFO 06-25 12:07:47 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-900...
[1;36m(VllmWorkerProcess pid=829423)[0;0m [1;36m(VllmWorkerProcess pid=829425)[0;0m INFO 06-25 12:07:47 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-900...
[1;36m(VllmWorkerProcess pid=829424)[0;0m INFO 06-25 12:07:47 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-900...
INFO 06-25 12:07:47 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-900...
[1;36m(VllmWorkerProcess pid=829425)[0;0m INFO 06-25 12:07:48 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 12:07:48 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=829423)[0;0m INFO 06-25 12:07:48 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=829424)[0;0m INFO 06-25 12:07:48 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 12:07:48 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 12:07:48 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=829424)[0;0m INFO 06-25 12:07:53 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=829424)[0;0m INFO 06-25 12:07:53 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=829423)[0;0m INFO 06-25 12:07:53 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=829423)[0;0m INFO 06-25 12:07:53 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:07:53 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=829425)[0;0m INFO 06-25 12:07:53 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-25 12:07:53 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=829425)[0;0m INFO 06-25 12:07:53 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:08:06 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=829425)[0;0m INFO 06-25 12:08:06 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=829423)[0;0m INFO 06-25 12:08:06 model_runner.py:1530] Graph capturing finished in 13 secs.
[1;36m(VllmWorkerProcess pid=829424)[0;0m INFO 06-25 12:08:06 model_runner.py:1530] Graph capturing finished in 13 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 3, 'acc': 37.5, 'total_acc': 37.5}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-900/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
37.5   	37.5   
INFO 06-25 12:09:07 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=829423)[0;0m [1;36m(VllmWorkerProcess pid=829425)[0;0m INFO 06-25 12:09:07 multiproc_worker_utils.py:242] Worker exiting
INFO 06-25 12:09:07 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=829424)[0;0m INFO 06-25 12:09:07 multiproc_worker_utils.py:242] Worker exiting
Random seed set as 0
INFO 06-25 12:09:33 config.py:887] Defaulting to use mp for distributed inference
INFO 06-25 12:09:33 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-final', speculative_config=None, tokenizer='/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-final', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-final, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
WARNING 06-25 12:09:33 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 255 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-25 12:09:33 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=830363)[0;0m INFO 06-25 12:09:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=830362)[0;0m INFO 06-25 12:09:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
[1;36m(VllmWorkerProcess pid=830364)[0;0m INFO 06-25 12:09:34 multiproc_worker_utils.py:216] Worker ready; awaiting tasks
INFO 06-25 12:09:37 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=830362)[0;0m INFO 06-25 12:09:37 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 12:09:37 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=830362)[0;0m INFO 06-25 12:09:37 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=830363)[0;0m [1;36m(VllmWorkerProcess pid=830364)[0;0m INFO 06-25 12:09:37 utils.py:1008] Found nccl from library libnccl.so.2
INFO 06-25 12:09:37 utils.py:1008] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=830363)[0;0m INFO 06-25 12:09:37 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=830364)[0;0m INFO 06-25 12:09:37 pynccl.py:63] vLLM is using nccl==2.20.5
[1;36m(VllmWorkerProcess pid=830362)[0;0m WARNING 06-25 12:09:37 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-25 12:09:37 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=830363)[0;0m WARNING 06-25 12:09:37 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorkerProcess pid=830364)[0;0m WARNING 06-25 12:09:37 custom_all_reduce.py:132] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-25 12:09:37 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f22e10df040>, local_subscribe_port=35493, remote_subscribe_port=None)
INFO 06-25 12:09:37 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-final...
[1;36m(VllmWorkerProcess pid=830362)[0;0m INFO 06-25 12:09:37 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-final...
[1;36m(VllmWorkerProcess pid=830363)[0;0m INFO 06-25 12:09:37 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-final...
[1;36m(VllmWorkerProcess pid=830364)[0;0m INFO 06-25 12:09:37 model_runner.py:1060] Starting to load model /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-final...
INFO 06-25 12:09:37 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=830362)[0;0m INFO 06-25 12:09:37 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=830363)[0;0m INFO 06-25 12:09:37 model_runner.py:1071] Loading model weights took 0.7654 GB
[1;36m(VllmWorkerProcess pid=830364)[0;0m INFO 06-25 12:09:38 model_runner.py:1071] Loading model weights took 0.7654 GB
INFO 06-25 12:09:39 distributed_gpu_executor.py:57] # GPU blocks: 185441, # CPU blocks: 18724
INFO 06-25 12:09:39 distributed_gpu_executor.py:61] Maximum concurrency for 4096 tokens per request: 724.38x
[1;36m(VllmWorkerProcess pid=830364)[0;0m INFO 06-25 12:09:43 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=830364)[0;0m INFO 06-25 12:09:43 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[1;36m(VllmWorkerProcess pid=830362)[0;0m INFO 06-25 12:09:43 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=830362)[0;0m INFO 06-25 12:09:43 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:09:43 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=830363)[0;0m INFO 06-25 12:09:43 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:09:43 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[1;36m(VllmWorkerProcess pid=830363)[0;0m INFO 06-25 12:09:43 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-25 12:10:01 model_runner.py:1530] Graph capturing finished in 18 secs.
[1;36m(VllmWorkerProcess pid=830362)[0;0m INFO 06-25 12:10:01 model_runner.py:1530] Graph capturing finished in 18 secs.
[1;36m(VllmWorkerProcess pid=830363)[0;0m INFO 06-25 12:10:01 model_runner.py:1530] Graph capturing finished in 18 secs.
[1;36m(VllmWorkerProcess pid=830364)[0;0m INFO 06-25 12:10:01 model_runner.py:1530] Graph capturing finished in 18 secs.
data_name: amc23x8
data_file: ./data/amc23x8/test.jsonl
==================================================
data: amc23x8  ,remain samples: 320
{'idx': 0, 'id': 0, 'problem': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?', 'answer': 27.0, 'url': 'https://artofproblemsolving.com/wiki/index.php/2023_AMC_12A_Problems/Problem_1', 'question': 'Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?'}
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
Cities $A$ and $B$ are $45$ miles apart. Alicia lives in $A$ and Beth lives in $B$. Alicia bikes towards $B$ at 18 miles per hour. Leaving at the same time, Beth bikes toward $A$ at 12 miles per hour. How many miles from City $A$ will they be when they meet?<|im_end|>
<|im_start|>assistant

-------------------- Epoch 0
-------------------- Epoch 1
Unsolved samples: 0
{'num_samples': 320, 'num_scores': 320, 'timeout_samples': 0, 'empty_samples': 29, 'acc': 31.6, 'total_acc': 31.5625}
Saved to /homes/gws/lxh22/rl-sft/one-input-sft/save/Qwen2.5-Math-1.5B/step-final/temp01/amc-eval/amc23x8/test_qwen25-math-cot_-1_seed0_t0.6_s0_e-1.jsonl
amc23x8	avg    
31.6   	31.6   
INFO 06-25 12:10:55 multiproc_worker_utils.py:134] Terminating local vLLM worker processes
[1;36m(VllmWorkerProcess pid=830362)[0;0m INFO 06-25 12:10:55 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=830363)[0;0m INFO 06-25 12:10:55 multiproc_worker_utils.py:242] Worker exiting
[1;36m(VllmWorkerProcess pid=830364)[0;0m INFO 06-25 12:10:55 multiproc_worker_utils.py:242] Worker exiting
✅  Finished submitting all checkpoints to eval_all_math.sh
